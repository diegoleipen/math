{"cells":[{"cell_type":"markdown","metadata":{"id":"MZFj73bKWHVk"},"source":["El objetivo de este notebook es implementar una red neuronal simplicial como se presentan en el articulo *Simplicial Neural Networks* de Stefania Ebli y Michaël Defferrard.\n","\n","El Semantic Scholar Open Research Corpus (SSORC) es una base de datos que contiene informacion de mas de 39 millones de articulos academicos. Para nuestros objetivos, conviene pensar en la base de datos como un conjunto donde cada elemento es un diccionario asociado a un articulo con la siguiente estructura\n","\n","```\n","paper = {\n","  'pid': el id del articulo,\n","  'authors': lista de los ids de los autores,\n","  '#cits': numero de veces que ha sido citado\n","}\n","```\n","\n","En la base de datos real, cada diccionario contiene mas informacion (items) pero en la implementacion, esta solo se ocupa para hacer downsampling. Por ejemplo, se quitan todos los articulos que satisfacen una de las siguientes condiciones (entre otras):\n","\n","* Falta el año de publicacion.\n","* Alguno de los autores tiene mas de un id asociado.\n","* No tienen referencias (i.e. no citan ningun otro articulo).\n","* Tienen mas de 10 autores.\n","\n","Cabe recalcar que el downsampling requiere cuidado y es parte del objetivo de los scripts s2_2 y s2_4 del github.\n","\n","El complejo de coautores $C$ es el complejo simplicial donde un\n","articulo con k autores esta representado por un $(k−1)$-simplejo.\n","Estamos interesados en las cocadenas dadas por $c(\\{v_0,...,v_p\\})$ = el número\n","total de citas de articulos que contienen a $v_0,...,v_p$ como autores.\n","\n","El objetivo del preprocesamiento de datos es\n","1. Obtener subscomplejos de $C$ asociados a una seed = paper y a una cantidad fija de articulos n_papers.\n","2. Calcular la cocadena que nos interesa en cada subcomplejo como una lista de tensores de dimension $(1, M_d)$ con $M_d$ = el numero de simplejos de dimension $d$ en el subcomplejo.\n","3. Dañar la cocadena obtenida en el inciso anterior. Especificamente, reemplazar una cantidad fija de valores por la mediana de los valores que no quitamos.\n","4. Calcular la mascara de los valores que quitamos. Basicamente es una forma sistematica de saber que valores son reales y cuales son la media (para evaluar).\n","5. Calcular las matrices laplacianas y las matrices asociadas a los operadores frontera (para el subcomplejo fijo).\n","\n","Por simplicidad, explicamos los detalles del preprocesamiento en el otro notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6376,"status":"ok","timestamp":1748800008590,"user":{"displayName":"Diego Leipen Lara","userId":"13333550093048231904"},"user_tz":360},"id":"O9fufjQ3xTpp","outputId":"de9444f4-a2d9-4164-9a6f-c5478186fedf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: gudhi in /usr/local/lib/python3.11/dist-packages (3.11.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from gudhi) (2.0.2)\n"]}],"source":["pip install gudhi"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4O4fytQs7uAm"},"outputs":[],"source":["import copy\n","import gudhi\n","import scipy\n","import torch\n","import random\n","import string\n","\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import torch.nn as nn"]},{"cell_type":"markdown","metadata":{"id":"FeBqUsFThRjX"},"source":["# SSORC de juguete\n","\n","* 5000 articulos\n","* 1000 autores\n","* 1-3 autores por articulo\n","* 1-20 citas por articulo\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f_X_qn5Rzi-8"},"outputs":[],"source":["def random_id(prefix, length):\n","    return prefix + ''.join(random.choices(string.ascii_uppercase + string.digits, k=length))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PY3iozflzhOM"},"outputs":[],"source":["paper_ids = list(set(random_id('P', 6) for _ in range(5000)))\n","author_ids = list(set(random_id('A', 6) for _ in range(1000)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sMkgnE23zc9h"},"outputs":[],"source":["ssorc = []\n","for pid in paper_ids:\n","  authors = random.sample(author_ids, random.randint(1, 3))\n","  num_cits = random.randint(1, 20)\n","  paper = {\n","      'pid': pid,\n","      'authors': authors,\n","      '#cits': num_cits\n","  }\n","  ssorc.append(paper)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":201},"executionInfo":{"elapsed":127,"status":"ok","timestamp":1748800015852,"user":{"displayName":"Diego Leipen Lara","userId":"13333550093048231904"},"user_tz":360},"id":"U7XzfXvogILr","outputId":"b68de279-7f99-4a3f-fc44-ea974418ecca"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"edges\",\n  \"rows\": 10075,\n  \"fields\": [\n    {\n      \"column\": \"paper\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"PNVYQSG\",\n          \"PORMI6L\",\n          \"PZ76FW2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"author\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          \"A8H2DOM\",\n          \"A67NBKL\",\n          \"A8ZUKXX\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"edges"},"text/html":["\n","  <div id=\"df-351d58bb-adf8-44b4-80c2-941f907eeaeb\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>paper</th>\n","      <th>author</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>PUHJCG9</td>\n","      <td>ALXBINP</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>PUHJCG9</td>\n","      <td>AT0DCEQ</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>PUHJCG9</td>\n","      <td>AB5VJYH</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>P2NS3MR</td>\n","      <td>AFFPAXN</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>P2NS3MR</td>\n","      <td>A8C6N4J</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-351d58bb-adf8-44b4-80c2-941f907eeaeb')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-351d58bb-adf8-44b4-80c2-941f907eeaeb button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-351d58bb-adf8-44b4-80c2-941f907eeaeb');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-66e7b13c-f27d-4a76-ac9e-770bfc53150e\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-66e7b13c-f27d-4a76-ac9e-770bfc53150e')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-66e7b13c-f27d-4a76-ac9e-770bfc53150e button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["     paper   author\n","0  PUHJCG9  ALXBINP\n","1  PUHJCG9  AT0DCEQ\n","2  PUHJCG9  AB5VJYH\n","3  P2NS3MR  AFFPAXN\n","4  P2NS3MR  A8C6N4J"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["edges = [\n","    (paper['pid'], author)\n","    for paper in ssorc\n","    for author in paper['authors']\n","]\n","edges = pd.DataFrame(data=edges, columns=['paper','author'])\n","edges.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1748800015858,"user":{"displayName":"Diego Leipen Lara","userId":"13333550093048231904"},"user_tz":360},"id":"C8WAcJgX4gWD","outputId":"8907fc1e-c7b3-4d52-9a66-8d7ea4d250a3"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"summary":"{\n  \"name\": \"papers\",\n  \"rows\": 5000,\n  \"fields\": [\n    {\n      \"column\": \"pid\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5000,\n        \"samples\": [\n          \"PNVYQSG\",\n          \"PORMI6L\",\n          \"PZ76FW2\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"#cits\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5,\n        \"min\": 1,\n        \"max\": 20,\n        \"num_unique_values\": 20,\n        \"samples\": [\n          1,\n          11,\n          17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}","type":"dataframe","variable_name":"papers"},"text/html":["\n","  <div id=\"df-125007e6-45d7-4940-a68f-d546b785ad45\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>#cits</th>\n","    </tr>\n","    <tr>\n","      <th>pid</th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>PUHJCG9</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>P2NS3MR</th>\n","      <td>12</td>\n","    </tr>\n","    <tr>\n","      <th>P6ZDTPT</th>\n","      <td>20</td>\n","    </tr>\n","    <tr>\n","      <th>PSVXHS4</th>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>PZNBNX9</th>\n","      <td>6</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-125007e6-45d7-4940-a68f-d546b785ad45')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-125007e6-45d7-4940-a68f-d546b785ad45 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-125007e6-45d7-4940-a68f-d546b785ad45');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-a53850a4-b5cc-42f5-af5d-001997a5b46d\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a53850a4-b5cc-42f5-af5d-001997a5b46d')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-a53850a4-b5cc-42f5-af5d-001997a5b46d button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"text/plain":["         #cits\n","pid           \n","PUHJCG9      1\n","P2NS3MR     12\n","P6ZDTPT     20\n","PSVXHS4     13\n","PZNBNX9      6"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["papers = {paper['pid']: paper['#cits'] for paper in ssorc}\n","papers = pd.DataFrame.from_dict(data=papers, orient='index', columns=['#cits'])\n","papers.index.name = 'pid'\n","papers.head()"]},{"cell_type":"markdown","source":["# Preprocesamiento\n","\n","Descripcion de la funcion preprocessing: (los detalles se encuentran en el otro notebook)\n","\n","Input:\n","\n","* papers (data frame de pandas como se visualiza arriba)\n","* edges (data frame de pandas como se visualiza arriba)\n","* seed (un pid aleatorio)\n","* n_papers (el numero maximo de papers que permitimos en el subcomplejo - el cual se genera con BFS en la grafica edges)\n","* percent (el porcentaje de papers que quitamos al dañar la cocadena)\n","\n","Output: Una lista de diccionarios (uno por seed) con los siguientes items\n","\n","*\tmask: una lista de listas (una por dimension) de indices que indican los valores no dañados.\n","*\ttarget: lista de tensores (uno por dimension) que representan la cocadena *real* restringida al subcomplejo.\n","*\txs: lista de tensores (uno por dimension) que representan la cocadena dañada.\n","*\tLs: listas de matrices laplacianas normalizadas (una por dimension) asociadas al subcomplejo."],"metadata":{"id":"DcEC_IcXnObA"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"muuaPhLxBBCo"},"outputs":[],"source":["def preprocessing(papers, edges, seed, n_papers, percent):\n","\n","  new_papers = [seed]\n","  keep_papers = {seed}\n","\n","  while len(keep_papers) < n_papers:\n","    new_authors = edges['author'][edges['paper'].isin(new_papers)]\n","    new_papers = edges['paper'][edges['author'].isin(new_authors)]\n","\n","    new_papers_set = set(new_papers.values) - keep_papers\n","    if not new_papers_set:\n","      break\n","\n","    keep_papers.update(new_papers_set)\n","    new_papers = list(new_papers_set)\n","\n","  papers = papers.loc[list(keep_papers)]\n","  papers = papers.sort_index()\n","\n","  edges = edges[edges['paper'].isin(papers.index)]\n","  edges = edges.sort_values('paper')\n","  edges = edges.reset_index(drop=True)\n","\n","  authors = pd.DataFrame(edges['author'].unique(), columns=['author'])\n","  authors.sort_values('author', inplace=True)\n","  authors.set_index('author', inplace=True, verify_integrity=True)\n","  authors['aidx'] = np.arange(len(authors))\n","\n","  papers['pidx'] = np.arange(len(papers))\n","\n","  edges = edges.join(papers['pidx'], on='paper', how='right')\n","  edges = edges.join(authors['aidx'], on='author', how='right')\n","  edges.sort_index(inplace=True)\n","\n","  biadjacency = scipy.sparse.coo_matrix(\n","      (np.ones(len(edges), dtype=bool),\n","      (edges['pidx'], edges['aidx']))\n","  )\n","\n","  biadj_lil = biadjacency.tolil()\n","\n","  st = gudhi.SimplexTree()\n","  for authors in biadj_lil.rows:\n","    st.insert(authors)\n","\n","  D = st.dimension()\n","\n","  pap_cits_by_authors = [dict() for _ in range(D+1)]\n","  for j, authors in enumerate(biadj_lil.rows):\n","    k = len(authors)\n","    pap_cits_by_authors[k-1].setdefault(frozenset(authors),[]).append(papers['#cits'].iloc[j])\n","\n","  precochain = [dict() for _ in range(D+1)]\n","  for d in range(len(pap_cits_by_authors)-1, -1, -1):\n","    for simplex, values in pap_cits_by_authors[d].items():\n","      st_simplex = gudhi.SimplexTree()\n","      st_simplex.insert(simplex)\n","      for face, _ in st_simplex.get_skeleton(st_simplex.dimension()):\n","        face = frozenset(face)\n","        precochain[len(face)-1].setdefault(face, []).extend(pap_cits_by_authors[d][simplex])\n","\n","  cochain = [dict() for _ in range(D+1)]\n","  for d in range(len(pap_cits_by_authors)-1, -1, -1):\n","    for simplex, values in pap_cits_by_authors[d].items():\n","      st_simplex = gudhi.SimplexTree()\n","      st_simplex.insert(simplex)\n","      for face, _ in st_simplex.get_skeleton(st_simplex.dimension()):\n","        face = frozenset(face)\n","        value = np.array(precochain[len(face)-1][face])\n","        cochain[len(face)-1][face] = int(np.sum(value))\n","\n","  simplices = [dict() for _ in range(D+1)]\n","  for simplex, _ in st.get_skeleton(D):\n","    k = len(simplex)-1\n","    simplices[k][frozenset(simplex)] = len(simplices[k])\n","\n","  simplices_list = [[None] * len(simplices[d]) for d in range(D+1)]\n","  for d in range(D+1):\n","    for simplex, idx in simplices[d].items():\n","      simplices_list[d][idx] = simplex\n","\n","  subsimplices = [dict() for _ in range(D+1)]\n","  for d in range(D+1):\n","    d_simp_list = list(simplices[d].keys())\n","    m = int(np.ceil((len(d_simp_list)/100)*percent))\n","    random.shuffle(d_simp_list)\n","    subsimplices_list = d_simp_list[:m]\n","    for simplex in subsimplices_list:\n","      subsimplices[d][simplex] = simplices[d][simplex]\n","\n","  mask = [list() for _ in range(D+1)]\n","  for d in range(D+1):\n","    known_simplices = list(set(simplices[d].keys()) - set(subsimplices[d].keys()))\n","    for simplex in known_simplices:\n","      mask[d].append(simplices[d][simplex])\n","\n","  median_random = []\n","  for d in range(D+1):\n","    known_simplices = list(set(simplices[d].keys()) - set(subsimplices[d].keys()))\n","    median_random.append(np.median([cochain[d][simplex] for simplex in known_simplices]))\n","\n","  damaged_cochain = copy.deepcopy(cochain)\n","  for d in range(D+1):\n","    for simplex in subsimplices[d].keys():\n","      damaged_cochain[d][simplex] = median_random[d]\n","\n","  cochain_values = []\n","  for d in range(D+1):\n","    cochain_values_d = []\n","    for idx in range(len(simplices[d])):\n","      cochain_values_d.append(cochain[d][simplices_list[d][idx]])\n","    cochain_values.append(cochain_values_d)\n","\n","  cochain_tensors = []\n","  for d in range(D+1):\n","    cochain_tensor = torch.zeros((1, len(cochain_values[d])), dtype=torch.float, requires_grad=False)\n","    cochain_tensor[0, :] = torch.tensor(cochain_values[d], dtype=torch.float, requires_grad=False)\n","    cochain_tensors.append(cochain_tensor)\n","\n","  damaged_cochain_values = []\n","  for d in range(D+1):\n","    damaged_cochain_values_d = []\n","    for idx in range(len(simplices[d])):\n","      damaged_cochain_values_d.append(damaged_cochain[d][simplices_list[d][idx]])\n","    damaged_cochain_values.append(damaged_cochain_values_d)\n","\n","  damaged_cochain_tensors = []\n","  for d in range(D+1):\n","    damaged_cochain_tensor = torch.zeros((1, len(damaged_cochain_values[d])), dtype=torch.float, requires_grad=False)\n","    damaged_cochain_tensor[0, :] = torch.tensor(damaged_cochain_values[d], dtype=torch.float, requires_grad=False)\n","    damaged_cochain_tensors.append(damaged_cochain_tensor)\n","\n","  boundaries = []\n","  for d in range(1, D+1):\n","    values, idx_simplices, idx_faces = [], [], []\n","    for simplex, idx_simplex in simplices[d].items():\n","      for i, left_out in enumerate(np.sort(list(simplex))):\n","        values.append((-1)**i)\n","        idx_simplices.append(idx_simplex)\n","        idx_faces.append(simplices[d-1][simplex.difference({left_out})])\n","    assert len(values) == (d+1) * len(simplices[d])\n","    boundary = scipy.sparse.coo_matrix(\n","        (values, (idx_faces, idx_simplices)),\n","        dtype = np.float32,\n","        shape = (len(simplices[d-1]), len(simplices[d]))\n","    )\n","    boundaries.append(boundary)\n","\n","  laplacians = list()\n","  up = scipy.sparse.coo_matrix(boundaries[0] @ boundaries[0].T)\n","  laplacians.append(up)\n","  for d in range(len(boundaries)-1):\n","    down = boundaries[d].T @ boundaries[d]\n","    up = boundaries[d+1] @ boundaries[d+1].T\n","    laplacians.append(scipy.sparse.coo_matrix(down + up))\n","  down = boundaries[-1].T @ boundaries[-1]\n","  laplacians.append(scipy.sparse.coo_matrix(down))\n","\n","  for i in range(len(laplacians)):\n","    L = laplacians[i]\n","    bigeig = scipy.sparse.linalg.eigsh(L, k=1, which='LM', return_eigenvectors=False)[0]\n","    laplacians[i] = L * (1.0 / bigeig)\n","\n","  def coo2tensor(A):\n","    idxs = torch.LongTensor(np.vstack((A.row, A.col)))\n","    vals = torch.FloatTensor(A.data)\n","    return torch.sparse_coo_tensor(idxs, vals, size=A.shape, requires_grad=False)\n","\n","  Ls = [coo2tensor(laplacians[d]) for d in range(D+1)]\n","\n","  sample = {\n","        'mask': mask,\n","        'target': cochain_tensors,\n","        'xs': damaged_cochain_tensors,\n","        'Ls': Ls,\n","  }\n","\n","  return sample"]},{"cell_type":"markdown","metadata":{"id":"j4OUGnXBfYVm"},"source":["# Nuestra base de datos (procesada)\n","\n","* 500 articulos es la cota de articulos para los subcomplejos.\n","* 20% es el porcentaje de articulos que quitamos al dañar.\n","\n","* 250 semillas para generar los subcomplejos asociados.\n","* 200 de estas son para entrenar.\n","* 50 para validar.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMddABqD-SFj"},"outputs":[],"source":["n_papers = 500\n","percent = 20\n","\n","n_samples = 250\n","seeds = random.choices(paper_ids, k=n_samples)\n","train_seeds = seeds[:200]\n","val_seeds = seeds[200:]\n","\n","train_data = [preprocessing(papers, edges, seed, n_papers, percent) for seed in train_seeds]\n","val_data = [preprocessing(papers, edges, seed, n_papers, percent) for seed in val_seeds]"]},{"cell_type":"markdown","metadata":{"id":"RN8skXFItsRo"},"source":["Sea $d$ una dimension fija y denotamos un simplejo arbitrario de dimension $d$ por $\\sigma_m$. Recordemos que la convolucion esta dada por\n","\n","$$\n","\\mathcal{F}_p^{-1}(\\varphi_\\theta) \\ast_p c = \\sum_{i=0}^I \\sum_{k=0}^K \\theta_{i,k} T_k(\\tilde{L}_d) c_i\n","$$\n","donde $I$ es el numero de input channels,  $T_k$ es el $k$-esimo polinomio de Chebyshev, $\\tilde{L}_d$ es la matriz laplaciana de dimension $d$ normalizada, y $c_i = (c(\\sigma_m)_i)_m$ es la matriz (vector columna) asociada a $c_i = \\pi_i \\circ c$.\n","\n","\n","La estructura de la siguiente funcion esta dada por:\n","\n","Input:\n","* $x$ es un $(I,M)$ tensor (donde $M$ es el numero de simplejos de dimension $d$) con $x[i][m]$ = el valor de la cocadena en el $i$-esimo canal y $m$-esimo simplejo de dimension $d$.\n","* $L$ es la matriz laplaciana normalizada de dimension $d$.\n","* $K$ es el grado del polinomio de Chebyshev.\n","\n","Output:\n","* $X$ es un $(I,M,K)$ tensor con $X[i][m][k] = T_k(\\tilde{L}_d)c_i$ donde $c_i$ es el vector columna $(x[i][m])_m$ y ."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f-zPeQzXkM-5"},"outputs":[],"source":["def chebyshev(x, L, K):\n","\n","  (I, M) = x.shape\n","\n","  X = []\n","\n","  for i in range(I):\n","    Xi = []\n","    Xi.append(x[i, :].unsqueeze(1))\n","    if K > 1:\n","      Xi.append(L.mm(Xi[0]))\n","    for k in range(2,K):\n","      Xi.append(2*(L.mm(Xi[k-1])) - Xi[k-2])\n","    Xi = torch.cat(Xi,1)\n","    X.append(Xi.unsqueeze(0))\n","\n","  X = torch.cat(X,0)\n","\n","  assert(X.shape == (I, M, K))\n","\n","  return X"]},{"cell_type":"markdown","metadata":{"id":"7SkOQPg_CMO2"},"source":["# Capas convolucionales\n","\n","Sea $d$ una dimension fija. Sea $I$ el numero de input channels, $O$ el numero de output channels, $K$ el grado del polinomio de Chebyshev, y $M$ el numero de simplejos de dimension $d$. Para un tensor $x_d \\in \\mathbb{R}^{I \\times M}$\n","\n","$$\n","C^d(x_d, \\tilde{L}_d)[o, m] = \\sum_{i=0}^{I-1} \\sum_{k=0}^{K-1} \\theta^{(d)}_{o,i,k} \\left[ T_k(\\tilde{L}_d) c_i \\right]_m + b_o \\in \\mathbb{R}^{O \\times M}\n","$$\n","\n","Por simplicidad, en lo que sigue omitimos la $d$ pero es importante recordar que estas capas convolucionales operan sobre una dimension simplicial fija.\n","\n","Es natural pensar en $C$ como una funcion de $\\mathbb{R}^{I \\times M}$ a $\\mathbb{R}^{O \\times M}$ (formalmente, esto no es preciso pues los parametros no estan fijos). En este caso, denotamos $C = C^{I,O}$. Esta notacion sera útil para describir la arquitectura modular de la red neuronal, donde cada capa actúa como una transformacion entre espacios de cocadenas de distintas dimensiones.\n","\n","La funcion forward de la clase SimplicialConvolution regresa $C(x, \\tilde{L})$ dado el input $(x, \\tilde{L})$. De esta manera, la red (definida en la clase MySCNN a continuacion) aprende\n","1. Filtros convolucionales de dimension $(O, I, K)$.\n","2. Sesgos de dimension $(O, 1)$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDD-0oL6vnlj"},"outputs":[],"source":["class SimplicialConvolution(nn.Module):\n","\n","  def __init__(self, I, O, K):\n","\n","    super().__init__()\n","\n","    self.I = I\n","    self.O = O\n","    self.K = K\n","\n","    self.theta = nn.parameter.Parameter(torch.randn(self.O, self.I, self.K))\n","    self.bias = nn.Parameter(torch.zeros(self.O))\n","\n","  def forward(self, x, L):\n","\n","    X = chebyshev(x, L, self.K)\n","    y = torch.einsum('imk,oik->om', (X, self.theta))\n","\n","    return y + self.bias.view(-1, 1)"]},{"cell_type":"markdown","metadata":{"id":"YSwJJsAUG0IB"},"source":["# Estructura de la red\n","\n","Para cada dimension $d = 0, 1, 2$, se aplican tres capas secuenciales de convolucion simplicial:\n","\n","$$\n","C_1 \\rightarrow \\text{ReLU} \\rightarrow C_2 \\rightarrow \\text{ReLU} \\rightarrow C_3\n","$$\n","\n","Mas precisamente, (y especificando las dimensiones) la red neuronal opera asi:\n","\n","$$\n","\\mathbb{R}^{I \\times M} \\xrightarrow{C^{I,O}} \\mathbb{R}^{O \\times M} \\xrightarrow{\\text{ReLU}} \\mathbb{R}^{O \\times M} \\xrightarrow{C^{O,O}} \\mathbb{R}^{O \\times M} \\xrightarrow{\\text{ReLU}} \\mathbb{R}^{O \\times M} \\xrightarrow{C^{O,I}} \\mathbb{R}^{I \\times M}\n","$$\n","\n","(sin activacion final)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOGPkIqdq_43"},"outputs":[],"source":["class MySCNN(nn.Module):\n","\n","  def __init__(self, colors=1):\n","\n","    super().__init__()\n","\n","    self.colors = colors\n","    num_filters = 10\n","    K = 5\n","\n","    self.C01 = SimplicialConvolution(colors, num_filters, K)\n","    self.C02 = SimplicialConvolution(num_filters, num_filters, K)\n","    self.C03 = SimplicialConvolution(num_filters, colors, K)\n","\n","    self.C11 = SimplicialConvolution(colors, num_filters, K)\n","    self.C12 = SimplicialConvolution(num_filters, num_filters, K)\n","    self.C13 = SimplicialConvolution(num_filters, colors, K)\n","\n","    self.C21 = SimplicialConvolution(colors, num_filters, K)\n","    self.C22 = SimplicialConvolution(num_filters, num_filters, K)\n","    self.C23 = SimplicialConvolution(num_filters, colors, K)\n","\n","  def forward(self, xs, Ls):\n","\n","    out0 = self.C01(xs[0], Ls[0])\n","    out1 = self.C11(xs[1], Ls[1])\n","    out2 = self.C21(xs[2], Ls[2])\n","\n","    out0 = self.C02(nn.LeakyReLU()(out0), Ls[0])\n","    out1 = self.C12(nn.LeakyReLU()(out1), Ls[1])\n","    out2 = self.C22(nn.LeakyReLU()(out2), Ls[2])\n","\n","    out0 = self.C03(nn.LeakyReLU()(out0), Ls[0])\n","    out1 = self.C13(nn.LeakyReLU()(out1), Ls[1])\n","    out2 = self.C23(nn.LeakyReLU()(out2), Ls[2])\n","\n","    return [out0, out1, out2]"]},{"cell_type":"markdown","metadata":{"id":"4U4gtJGaSiFI"},"source":["# Entrenamiento\n","\n","Ocupamos el mismo optimizador y criterio de perdida que en el github. Especificamente,\n","\n","```\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","criterion = nn.L1Loss(reduction='sum')\n","```\n","\n","Para cada sample en data, entrenamos el modelo n_epochs=20 epocas.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xmc5NKNssDp5"},"outputs":[],"source":["def train(data, n_epochs):\n","\n","  model = MySCNN(colors=1)\n","  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","  criterion = nn.L1Loss(reduction='sum')\n","\n","  for sample_idx, sample in enumerate(data):\n","    mask = sample['mask']\n","    target = sample['target']\n","    xs = sample['xs']\n","    Ls = sample['Ls']\n","\n","    print(f\"\\nTraining on Sample {sample_idx}\")\n","\n","    for epoch in range(n_epochs):\n","\n","      model.train()\n","      optimizer.zero_grad()\n","      ys = model(xs, Ls)\n","\n","      loss = torch.tensor(0.0)\n","      for d in range(3):\n","        loss += criterion(ys[d][0, mask[d]], target[d][0, mask[d]])\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      if epoch in [0, n_epochs - 1]:\n","        print(f\"  Epoch {epoch+1}/{n_epochs}, Loss: {loss.item():.4f}\")\n","\n","  return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"KAauPikfwNyQ","outputId":"02f27b7f-fbfd-46f9-f732-bd73182d9d96"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Training on Sample 0\n","  Epoch 1/20, Loss: 1091734.8750\n","  Epoch 20/20, Loss: 710788.6875\n","\n","Training on Sample 1\n","  Epoch 1/20, Loss: 488010.4688\n","  Epoch 20/20, Loss: 339703.1875\n","\n","Training on Sample 2\n","  Epoch 1/20, Loss: 847596.7500\n","  Epoch 20/20, Loss: 430110.4062\n","\n","Training on Sample 3\n","  Epoch 1/20, Loss: 183754.0625\n","  Epoch 20/20, Loss: 164911.2969\n","\n","Training on Sample 4\n","  Epoch 1/20, Loss: 326338.0000\n","  Epoch 20/20, Loss: 257349.1562\n","\n","Training on Sample 5\n","  Epoch 1/20, Loss: 233034.2344\n","  Epoch 20/20, Loss: 191583.2344\n","\n","Training on Sample 6\n","  Epoch 1/20, Loss: 278451.5625\n","  Epoch 20/20, Loss: 225196.6562\n","\n","Training on Sample 7\n","  Epoch 1/20, Loss: 213731.7812\n","  Epoch 20/20, Loss: 179906.7500\n","\n","Training on Sample 8\n","  Epoch 1/20, Loss: 87093.8203\n","  Epoch 20/20, Loss: 62725.1445\n","\n","Training on Sample 9\n","  Epoch 1/20, Loss: 211550.2188\n","  Epoch 20/20, Loss: 148363.6250\n","\n","Training on Sample 10\n","  Epoch 1/20, Loss: 137412.0938\n","  Epoch 20/20, Loss: 121661.0469\n","\n","Training on Sample 11\n","  Epoch 1/20, Loss: 82343.8438\n","  Epoch 20/20, Loss: 68201.9219\n","\n","Training on Sample 12\n","  Epoch 1/20, Loss: 35809.3711\n","  Epoch 20/20, Loss: 29850.9941\n","\n","Training on Sample 13\n","  Epoch 1/20, Loss: 78043.7734\n","  Epoch 20/20, Loss: 65085.2266\n","\n","Training on Sample 14\n","  Epoch 1/20, Loss: 25554.7949\n","  Epoch 20/20, Loss: 23898.2480\n","\n","Training on Sample 15\n","  Epoch 1/20, Loss: 46417.2305\n","  Epoch 20/20, Loss: 42276.1914\n","\n","Training on Sample 16\n","  Epoch 1/20, Loss: 72324.1562\n","  Epoch 20/20, Loss: 61869.0664\n","\n","Training on Sample 17\n","  Epoch 1/20, Loss: 101571.2969\n","  Epoch 20/20, Loss: 84262.5234\n","\n","Training on Sample 18\n","  Epoch 1/20, Loss: 77860.5547\n","  Epoch 20/20, Loss: 69686.1875\n","\n","Training on Sample 19\n","  Epoch 1/20, Loss: 56299.9961\n","  Epoch 20/20, Loss: 48615.0273\n","\n","Training on Sample 20\n","  Epoch 1/20, Loss: 43523.3711\n","  Epoch 20/20, Loss: 40293.7266\n","\n","Training on Sample 21\n","  Epoch 1/20, Loss: 54234.3711\n","  Epoch 20/20, Loss: 49125.5859\n","\n","Training on Sample 22\n","  Epoch 1/20, Loss: 30232.0059\n","  Epoch 20/20, Loss: 26813.4102\n","\n","Training on Sample 23\n","  Epoch 1/20, Loss: 50724.3594\n","  Epoch 20/20, Loss: 44988.0664\n","\n","Training on Sample 24\n","  Epoch 1/20, Loss: 30723.2344\n","  Epoch 20/20, Loss: 28370.3535\n","\n","Training on Sample 25\n","  Epoch 1/20, Loss: 80076.0625\n","  Epoch 20/20, Loss: 61228.2031\n","\n","Training on Sample 26\n","  Epoch 1/20, Loss: 44279.7891\n","  Epoch 20/20, Loss: 37574.3281\n","\n","Training on Sample 27\n","  Epoch 1/20, Loss: 68802.4766\n","  Epoch 20/20, Loss: 57506.8359\n","\n","Training on Sample 28\n","  Epoch 1/20, Loss: 46001.0312\n","  Epoch 20/20, Loss: 44080.9922\n","\n","Training on Sample 29\n","  Epoch 1/20, Loss: 44638.7422\n","  Epoch 20/20, Loss: 40874.6133\n","\n","Training on Sample 30\n","  Epoch 1/20, Loss: 34094.2109\n","  Epoch 20/20, Loss: 32159.0117\n","\n","Training on Sample 31\n","  Epoch 1/20, Loss: 32483.9707\n","  Epoch 20/20, Loss: 31207.9727\n","\n","Training on Sample 32\n","  Epoch 1/20, Loss: 24508.3164\n","  Epoch 20/20, Loss: 22849.5469\n","\n","Training on Sample 33\n","  Epoch 1/20, Loss: 19193.7246\n","  Epoch 20/20, Loss: 18117.2246\n","\n","Training on Sample 34\n","  Epoch 1/20, Loss: 37296.7539\n","  Epoch 20/20, Loss: 34193.0195\n","\n","Training on Sample 35\n","  Epoch 1/20, Loss: 50652.4531\n","  Epoch 20/20, Loss: 43164.7852\n","\n","Training on Sample 36\n","  Epoch 1/20, Loss: 31429.1680\n","  Epoch 20/20, Loss: 27147.0996\n","\n","Training on Sample 37\n","  Epoch 1/20, Loss: 30196.4199\n","  Epoch 20/20, Loss: 28739.1582\n","\n","Training on Sample 38\n","  Epoch 1/20, Loss: 35459.1562\n","  Epoch 20/20, Loss: 32729.5430\n","\n","Training on Sample 39\n","  Epoch 1/20, Loss: 13954.6104\n","  Epoch 20/20, Loss: 11128.9033\n","\n","Training on Sample 40\n","  Epoch 1/20, Loss: 13947.5205\n","  Epoch 20/20, Loss: 12337.4375\n","\n","Training on Sample 41\n","  Epoch 1/20, Loss: 20487.6621\n","  Epoch 20/20, Loss: 18902.1387\n","\n","Training on Sample 42\n","  Epoch 1/20, Loss: 52936.9727\n","  Epoch 20/20, Loss: 34906.3203\n","\n","Training on Sample 43\n","  Epoch 1/20, Loss: 37561.4453\n","  Epoch 20/20, Loss: 28563.6289\n","\n","Training on Sample 44\n","  Epoch 1/20, Loss: 20114.9023\n","  Epoch 20/20, Loss: 15771.4766\n","\n","Training on Sample 45\n","  Epoch 1/20, Loss: 24269.9668\n","  Epoch 20/20, Loss: 22313.6816\n","\n","Training on Sample 46\n","  Epoch 1/20, Loss: 16910.9766\n","  Epoch 20/20, Loss: 15272.0967\n","\n","Training on Sample 47\n","  Epoch 1/20, Loss: 21082.5508\n","  Epoch 20/20, Loss: 18991.8203\n","\n","Training on Sample 48\n","  Epoch 1/20, Loss: 27251.5898\n","  Epoch 20/20, Loss: 25198.8535\n","\n","Training on Sample 49\n","  Epoch 1/20, Loss: 34378.8984\n","  Epoch 20/20, Loss: 31991.0527\n","\n","Training on Sample 50\n","  Epoch 1/20, Loss: 24972.2266\n","  Epoch 20/20, Loss: 22375.5176\n","\n","Training on Sample 51\n","  Epoch 1/20, Loss: 25559.5547\n","  Epoch 20/20, Loss: 24493.9219\n","\n","Training on Sample 52\n","  Epoch 1/20, Loss: 23473.3359\n","  Epoch 20/20, Loss: 22354.9160\n","\n","Training on Sample 53\n","  Epoch 1/20, Loss: 19735.9883\n","  Epoch 20/20, Loss: 16848.4551\n","\n","Training on Sample 54\n","  Epoch 1/20, Loss: 29111.3789\n","  Epoch 20/20, Loss: 24025.6055\n","\n","Training on Sample 55\n","  Epoch 1/20, Loss: 20762.5996\n","  Epoch 20/20, Loss: 18910.0137\n","\n","Training on Sample 56\n","  Epoch 1/20, Loss: 13521.2285\n","  Epoch 20/20, Loss: 11879.6113\n","\n","Training on Sample 57\n","  Epoch 1/20, Loss: 22015.8867\n","  Epoch 20/20, Loss: 18144.7285\n","\n","Training on Sample 58\n","  Epoch 1/20, Loss: 19017.4590\n","  Epoch 20/20, Loss: 17298.7832\n","\n","Training on Sample 59\n","  Epoch 1/20, Loss: 15012.7773\n","  Epoch 20/20, Loss: 14178.7041\n","\n","Training on Sample 60\n","  Epoch 1/20, Loss: 39374.5078\n","  Epoch 20/20, Loss: 26448.6465\n","\n","Training on Sample 61\n","  Epoch 1/20, Loss: 25659.0137\n","  Epoch 20/20, Loss: 23114.0449\n","\n","Training on Sample 62\n","  Epoch 1/20, Loss: 16760.7012\n","  Epoch 20/20, Loss: 13084.4531\n","\n","Training on Sample 63\n","  Epoch 1/20, Loss: 31162.7793\n","  Epoch 20/20, Loss: 25648.9590\n","\n","Training on Sample 64\n","  Epoch 1/20, Loss: 18664.2871\n","  Epoch 20/20, Loss: 16251.2695\n","\n","Training on Sample 65\n","  Epoch 1/20, Loss: 11538.9160\n","  Epoch 20/20, Loss: 10247.0508\n","\n","Training on Sample 66\n","  Epoch 1/20, Loss: 40217.5742\n","  Epoch 20/20, Loss: 23045.9980\n","\n","Training on Sample 67\n","  Epoch 1/20, Loss: 23500.1348\n","  Epoch 20/20, Loss: 21815.3594\n","\n","Training on Sample 68\n","  Epoch 1/20, Loss: 18629.6738\n","  Epoch 20/20, Loss: 17792.8516\n","\n","Training on Sample 69\n","  Epoch 1/20, Loss: 13468.7520\n","  Epoch 20/20, Loss: 10602.8516\n","\n","Training on Sample 70\n","  Epoch 1/20, Loss: 15309.0293\n","  Epoch 20/20, Loss: 13037.4619\n","\n","Training on Sample 71\n","  Epoch 1/20, Loss: 25278.3223\n","  Epoch 20/20, Loss: 19836.4707\n","\n","Training on Sample 72\n","  Epoch 1/20, Loss: 17122.3613\n","  Epoch 20/20, Loss: 15328.2881\n","\n","Training on Sample 73\n","  Epoch 1/20, Loss: 24719.2734\n","  Epoch 20/20, Loss: 21383.2090\n","\n","Training on Sample 74\n","  Epoch 1/20, Loss: 19365.6074\n","  Epoch 20/20, Loss: 19570.6582\n","\n","Training on Sample 75\n","  Epoch 1/20, Loss: 20493.5664\n","  Epoch 20/20, Loss: 18952.1855\n","\n","Training on Sample 76\n","  Epoch 1/20, Loss: 21120.5254\n","  Epoch 20/20, Loss: 20399.9375\n","\n","Training on Sample 77\n","  Epoch 1/20, Loss: 16872.4785\n","  Epoch 20/20, Loss: 15366.3057\n","\n","Training on Sample 78\n","  Epoch 1/20, Loss: 14142.4727\n","  Epoch 20/20, Loss: 11717.2598\n","\n","Training on Sample 79\n","  Epoch 1/20, Loss: 15041.8291\n","  Epoch 20/20, Loss: 14595.3389\n","\n","Training on Sample 80\n","  Epoch 1/20, Loss: 16857.5508\n","  Epoch 20/20, Loss: 15226.5078\n","\n","Training on Sample 81\n","  Epoch 1/20, Loss: 17229.0137\n","  Epoch 20/20, Loss: 16111.9609\n","\n","Training on Sample 82\n","  Epoch 1/20, Loss: 15464.6748\n","  Epoch 20/20, Loss: 14132.2441\n","\n","Training on Sample 83\n","  Epoch 1/20, Loss: 12127.8203\n","  Epoch 20/20, Loss: 11409.8916\n","\n","Training on Sample 84\n","  Epoch 1/20, Loss: 15397.6172\n","  Epoch 20/20, Loss: 13653.4023\n","\n","Training on Sample 85\n","  Epoch 1/20, Loss: 14892.0742\n","  Epoch 20/20, Loss: 13864.1729\n","\n","Training on Sample 86\n","  Epoch 1/20, Loss: 19762.5469\n","  Epoch 20/20, Loss: 15672.9844\n","\n","Training on Sample 87\n","  Epoch 1/20, Loss: 13168.7080\n","  Epoch 20/20, Loss: 10811.7930\n","\n","Training on Sample 88\n","  Epoch 1/20, Loss: 6279.0542\n","  Epoch 20/20, Loss: 4383.2212\n","\n","Training on Sample 89\n","  Epoch 1/20, Loss: 7140.9692\n","  Epoch 20/20, Loss: 6003.3101\n","\n","Training on Sample 90\n","  Epoch 1/20, Loss: 13919.0273\n","  Epoch 20/20, Loss: 10574.3652\n","\n","Training on Sample 91\n","  Epoch 1/20, Loss: 17934.9082\n","  Epoch 20/20, Loss: 14909.5781\n","\n","Training on Sample 92\n","  Epoch 1/20, Loss: 17228.0273\n","  Epoch 20/20, Loss: 16072.5322\n","\n","Training on Sample 93\n","  Epoch 1/20, Loss: 13186.9932\n","  Epoch 20/20, Loss: 11059.0850\n","\n","Training on Sample 94\n","  Epoch 1/20, Loss: 13751.9053\n","  Epoch 20/20, Loss: 12463.6279\n","\n","Training on Sample 95\n","  Epoch 1/20, Loss: 13169.3369\n","  Epoch 20/20, Loss: 12127.6885\n","\n","Training on Sample 96\n","  Epoch 1/20, Loss: 12277.1279\n","  Epoch 20/20, Loss: 12359.7900\n","\n","Training on Sample 97\n","  Epoch 1/20, Loss: 11911.4482\n","  Epoch 20/20, Loss: 8215.6953\n","\n","Training on Sample 98\n","  Epoch 1/20, Loss: 26137.1816\n","  Epoch 20/20, Loss: 14425.2158\n","\n","Training on Sample 99\n","  Epoch 1/20, Loss: 15499.9121\n","  Epoch 20/20, Loss: 11120.8857\n","\n","Training on Sample 100\n","  Epoch 1/20, Loss: 14747.2637\n","  Epoch 20/20, Loss: 11661.4912\n","\n","Training on Sample 101\n","  Epoch 1/20, Loss: 11108.7266\n","  Epoch 20/20, Loss: 7859.2471\n","\n","Training on Sample 102\n","  Epoch 1/20, Loss: 19155.1719\n","  Epoch 20/20, Loss: 12315.0947\n","\n","Training on Sample 103\n","  Epoch 1/20, Loss: 11431.5430\n","  Epoch 20/20, Loss: 6662.1235\n","\n","Training on Sample 104\n","  Epoch 1/20, Loss: 10630.0811\n","  Epoch 20/20, Loss: 8454.3311\n","\n","Training on Sample 105\n","  Epoch 1/20, Loss: 10155.9072\n","  Epoch 20/20, Loss: 9441.9863\n","\n","Training on Sample 106\n","  Epoch 1/20, Loss: 9627.2373\n","  Epoch 20/20, Loss: 8974.0283\n","\n","Training on Sample 107\n","  Epoch 1/20, Loss: 8496.6025\n","  Epoch 20/20, Loss: 7137.5132\n","\n","Training on Sample 108\n","  Epoch 1/20, Loss: 18259.4473\n","  Epoch 20/20, Loss: 13638.0234\n","\n","Training on Sample 109\n","  Epoch 1/20, Loss: 11400.1240\n","  Epoch 20/20, Loss: 8717.0059\n","\n","Training on Sample 110\n","  Epoch 1/20, Loss: 12145.2139\n","  Epoch 20/20, Loss: 9686.3184\n","\n","Training on Sample 111\n","  Epoch 1/20, Loss: 11793.8613\n","  Epoch 20/20, Loss: 10983.9014\n","\n","Training on Sample 112\n","  Epoch 1/20, Loss: 10408.5635\n","  Epoch 20/20, Loss: 9224.4502\n","\n","Training on Sample 113\n","  Epoch 1/20, Loss: 11912.3701\n","  Epoch 20/20, Loss: 10875.0791\n","\n","Training on Sample 114\n","  Epoch 1/20, Loss: 7301.9023\n","  Epoch 20/20, Loss: 4735.0654\n","\n","Training on Sample 115\n","  Epoch 1/20, Loss: 21361.4141\n","  Epoch 20/20, Loss: 11587.9053\n","\n","Training on Sample 116\n","  Epoch 1/20, Loss: 11423.6543\n","  Epoch 20/20, Loss: 7664.3760\n","\n","Training on Sample 117\n","  Epoch 1/20, Loss: 10019.4346\n","  Epoch 20/20, Loss: 8811.0020\n","\n","Training on Sample 118\n","  Epoch 1/20, Loss: 9002.8232\n","  Epoch 20/20, Loss: 8550.9023\n","\n","Training on Sample 119\n","  Epoch 1/20, Loss: 7515.8760\n","  Epoch 20/20, Loss: 6012.7397\n","\n","Training on Sample 120\n","  Epoch 1/20, Loss: 14978.5039\n","  Epoch 20/20, Loss: 9878.9189\n","\n","Training on Sample 121\n","  Epoch 1/20, Loss: 9209.0820\n","  Epoch 20/20, Loss: 8558.0654\n","\n","Training on Sample 122\n","  Epoch 1/20, Loss: 11725.2637\n","  Epoch 20/20, Loss: 10523.1035\n","\n","Training on Sample 123\n","  Epoch 1/20, Loss: 9854.9600\n","  Epoch 20/20, Loss: 5695.9268\n","\n","Training on Sample 124\n","  Epoch 1/20, Loss: 24728.9746\n","  Epoch 20/20, Loss: 11503.5322\n","\n","Training on Sample 125\n","  Epoch 1/20, Loss: 5466.6592\n","  Epoch 20/20, Loss: 3291.6448\n","\n","Training on Sample 126\n","  Epoch 1/20, Loss: 5704.0923\n","  Epoch 20/20, Loss: 4480.3374\n","\n","Training on Sample 127\n","  Epoch 1/20, Loss: 19866.0059\n","  Epoch 20/20, Loss: 9926.2764\n","\n","Training on Sample 128\n","  Epoch 1/20, Loss: 6278.8955\n","  Epoch 20/20, Loss: 4200.8516\n","\n","Training on Sample 129\n","  Epoch 1/20, Loss: 5549.3159\n","  Epoch 20/20, Loss: 3957.7432\n","\n","Training on Sample 130\n","  Epoch 1/20, Loss: 10299.8203\n","  Epoch 20/20, Loss: 6171.4268\n","\n","Training on Sample 131\n","  Epoch 1/20, Loss: 5061.8965\n","  Epoch 20/20, Loss: 4547.6289\n","\n","Training on Sample 132\n","  Epoch 1/20, Loss: 5247.7593\n","  Epoch 20/20, Loss: 4422.0205\n","\n","Training on Sample 133\n","  Epoch 1/20, Loss: 19883.6328\n","  Epoch 20/20, Loss: 11742.1191\n","\n","Training on Sample 134\n","  Epoch 1/20, Loss: 12366.9141\n","  Epoch 20/20, Loss: 6001.3115\n","\n","Training on Sample 135\n","  Epoch 1/20, Loss: 4100.3481\n","  Epoch 20/20, Loss: 3627.5525\n","\n","Training on Sample 136\n","  Epoch 1/20, Loss: 12411.2822\n","  Epoch 20/20, Loss: 8157.6338\n","\n","Training on Sample 137\n","  Epoch 1/20, Loss: 6311.2402\n","  Epoch 20/20, Loss: 5427.6099\n","\n","Training on Sample 138\n","  Epoch 1/20, Loss: 6527.9644\n","  Epoch 20/20, Loss: 4201.5391\n","\n","Training on Sample 139\n","  Epoch 1/20, Loss: 17912.6113\n","  Epoch 20/20, Loss: 7792.6860\n","\n","Training on Sample 140\n","  Epoch 1/20, Loss: 7753.7588\n","  Epoch 20/20, Loss: 6707.4126\n","\n","Training on Sample 141\n","  Epoch 1/20, Loss: 6913.5903\n","  Epoch 20/20, Loss: 4614.3916\n","\n","Training on Sample 142\n","  Epoch 1/20, Loss: 5295.8291\n","  Epoch 20/20, Loss: 4818.0371\n","\n","Training on Sample 143\n","  Epoch 1/20, Loss: 4906.5137\n","  Epoch 20/20, Loss: 3683.9602\n","\n","Training on Sample 144\n","  Epoch 1/20, Loss: 4531.9727\n","  Epoch 20/20, Loss: 3937.0676\n","\n","Training on Sample 145\n","  Epoch 1/20, Loss: 16327.5615\n","  Epoch 20/20, Loss: 7585.4302\n","\n","Training on Sample 146\n","  Epoch 1/20, Loss: 8397.4941\n","  Epoch 20/20, Loss: 7488.5503\n","\n","Training on Sample 147\n","  Epoch 1/20, Loss: 7257.6245\n","  Epoch 20/20, Loss: 5960.5566\n","\n","Training on Sample 148\n","  Epoch 1/20, Loss: 8918.3320\n","  Epoch 20/20, Loss: 6716.1089\n","\n","Training on Sample 149\n","  Epoch 1/20, Loss: 8244.5156\n","  Epoch 20/20, Loss: 7123.4224\n","\n","Training on Sample 150\n","  Epoch 1/20, Loss: 6891.5825\n","  Epoch 20/20, Loss: 6515.6240\n","\n","Training on Sample 151\n","  Epoch 1/20, Loss: 6668.8179\n","  Epoch 20/20, Loss: 5894.2432\n","\n","Training on Sample 152\n","  Epoch 1/20, Loss: 6496.7104\n","  Epoch 20/20, Loss: 6057.9766\n","\n","Training on Sample 153\n","  Epoch 1/20, Loss: 6513.4248\n","  Epoch 20/20, Loss: 6735.6641\n","\n","Training on Sample 154\n","  Epoch 1/20, Loss: 6977.1021\n","  Epoch 20/20, Loss: 5505.2065\n","\n","Training on Sample 155\n","  Epoch 1/20, Loss: 6433.9810\n","  Epoch 20/20, Loss: 5213.6182\n","\n","Training on Sample 156\n","  Epoch 1/20, Loss: 6308.6440\n","  Epoch 20/20, Loss: 4949.1562\n","\n","Training on Sample 157\n","  Epoch 1/20, Loss: 5291.1978\n","  Epoch 20/20, Loss: 3846.0503\n","\n","Training on Sample 158\n","  Epoch 1/20, Loss: 11627.0137\n","  Epoch 20/20, Loss: 7503.9463\n","\n","Training on Sample 159\n","  Epoch 1/20, Loss: 6106.8433\n","  Epoch 20/20, Loss: 3603.5659\n","\n","Training on Sample 160\n","  Epoch 1/20, Loss: 16131.5615\n","  Epoch 20/20, Loss: 8289.1387\n","\n","Training on Sample 161\n","  Epoch 1/20, Loss: 7196.5059\n","  Epoch 20/20, Loss: 5681.8096\n","\n","Training on Sample 162\n","  Epoch 1/20, Loss: 6235.4292\n","  Epoch 20/20, Loss: 4229.9604\n","\n","Training on Sample 163\n","  Epoch 1/20, Loss: 5718.2993\n","  Epoch 20/20, Loss: 5137.1045\n","\n","Training on Sample 164\n","  Epoch 1/20, Loss: 5864.8862\n","  Epoch 20/20, Loss: 4784.8862\n","\n","Training on Sample 165\n","  Epoch 1/20, Loss: 6959.5703\n","  Epoch 20/20, Loss: 7286.5708\n","\n","Training on Sample 166\n","  Epoch 1/20, Loss: 6521.7041\n","  Epoch 20/20, Loss: 5010.8477\n","\n","Training on Sample 167\n","  Epoch 1/20, Loss: 5545.7622\n","  Epoch 20/20, Loss: 4566.9370\n","\n","Training on Sample 168\n","  Epoch 1/20, Loss: 9071.5400\n","  Epoch 20/20, Loss: 5668.7114\n","\n","Training on Sample 169\n","  Epoch 1/20, Loss: 5514.1699\n","  Epoch 20/20, Loss: 4756.6821\n","\n","Training on Sample 170\n","  Epoch 1/20, Loss: 5733.5889\n","  Epoch 20/20, Loss: 4362.0747\n","\n","Training on Sample 171\n","  Epoch 1/20, Loss: 11555.9248\n","  Epoch 20/20, Loss: 6351.9736\n","\n","Training on Sample 172\n","  Epoch 1/20, Loss: 5283.5664\n","  Epoch 20/20, Loss: 3497.9580\n","\n","Training on Sample 173\n","  Epoch 1/20, Loss: 8911.3232\n","  Epoch 20/20, Loss: 7321.5298\n","\n","Training on Sample 174\n","  Epoch 1/20, Loss: 5781.6992\n","  Epoch 20/20, Loss: 2986.8560\n","\n","Training on Sample 175\n","  Epoch 1/20, Loss: 4390.8320\n","  Epoch 20/20, Loss: 3164.0647\n","\n","Training on Sample 176\n","  Epoch 1/20, Loss: 8920.5693\n","  Epoch 20/20, Loss: 4551.6255\n","\n","Training on Sample 177\n","  Epoch 1/20, Loss: 3659.0603\n","  Epoch 20/20, Loss: 3440.3955\n","\n","Training on Sample 178\n","  Epoch 1/20, Loss: 6974.9766\n","  Epoch 20/20, Loss: 4868.1343\n","\n","Training on Sample 179\n","  Epoch 1/20, Loss: 5061.4907\n","  Epoch 20/20, Loss: 3987.8594\n","\n","Training on Sample 180\n","  Epoch 1/20, Loss: 4272.6401\n","  Epoch 20/20, Loss: 4008.6248\n","\n","Training on Sample 181\n","  Epoch 1/20, Loss: 4202.2402\n","  Epoch 20/20, Loss: 3919.7283\n","\n","Training on Sample 182\n","  Epoch 1/20, Loss: 4832.8296\n","  Epoch 20/20, Loss: 3144.8408\n","\n","Training on Sample 183\n","  Epoch 1/20, Loss: 5417.8281\n","  Epoch 20/20, Loss: 3920.2979\n","\n","Training on Sample 184\n","  Epoch 1/20, Loss: 4083.4221\n","  Epoch 20/20, Loss: 2916.4438\n","\n","Training on Sample 185\n","  Epoch 1/20, Loss: 11674.1123\n","  Epoch 20/20, Loss: 4032.8489\n","\n","Training on Sample 186\n","  Epoch 1/20, Loss: 5696.3638\n","  Epoch 20/20, Loss: 2792.7473\n","\n","Training on Sample 187\n","  Epoch 1/20, Loss: 5539.3076\n","  Epoch 20/20, Loss: 4022.8762\n","\n","Training on Sample 188\n","  Epoch 1/20, Loss: 9197.5215\n","  Epoch 20/20, Loss: 4544.6338\n","\n","Training on Sample 189\n","  Epoch 1/20, Loss: 5577.2402\n","  Epoch 20/20, Loss: 3636.6311\n","\n","Training on Sample 190\n","  Epoch 1/20, Loss: 5206.8589\n","  Epoch 20/20, Loss: 2945.9641\n","\n","Training on Sample 191\n","  Epoch 1/20, Loss: 3827.2644\n","  Epoch 20/20, Loss: 3550.0076\n","\n","Training on Sample 192\n","  Epoch 1/20, Loss: 5093.1367\n","  Epoch 20/20, Loss: 3331.0115\n","\n","Training on Sample 193\n","  Epoch 1/20, Loss: 3648.9429\n","  Epoch 20/20, Loss: 3292.3301\n","\n","Training on Sample 194\n","  Epoch 1/20, Loss: 3544.4819\n","  Epoch 20/20, Loss: 2540.0518\n","\n","Training on Sample 195\n","  Epoch 1/20, Loss: 11273.1709\n","  Epoch 20/20, Loss: 5980.9570\n","\n","Training on Sample 196\n","  Epoch 1/20, Loss: 6533.5420\n","  Epoch 20/20, Loss: 5926.8184\n","\n","Training on Sample 197\n","  Epoch 1/20, Loss: 5980.6621\n","  Epoch 20/20, Loss: 2272.9021\n","\n","Training on Sample 198\n","  Epoch 1/20, Loss: 3068.0457\n","  Epoch 20/20, Loss: 1794.8331\n","\n","Training on Sample 199\n","  Epoch 1/20, Loss: 13538.1484\n","  Epoch 20/20, Loss: 4517.1685\n"]}],"source":["trained_model = train(train_data, n_epochs=20)"]},{"cell_type":"markdown","source":["# Evaluacion\n","\n","Los conceptos de accuracy y absolute errors definidos en la funcion evaluate coinciden con los definidos en el articulo. Especificamente,\n","\n","*A missing value is considered to be correctly imputed if the imputed value differs by at most 10% from the true value. The accuracy is the percentage of missing values that has been correctly imputed and the absolute error is the magnitude of the difference between the imputed and true value.*\n"],"metadata":{"id":"a9hEbPf0lDvR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_DNe2WV3NG8y"},"outputs":[],"source":["def evaluate(model, data, criterion):\n","\n","  total_loss = 0.0\n","  total_abs_error = 0.0\n","  total_correct = 0\n","  total_count = 0\n","\n","  model.eval()\n","\n","  with torch.no_grad():\n","\n","    for sample in data:\n","\n","      mask = sample['mask']\n","      target = sample['target']\n","      xs = sample['xs']\n","      Ls = sample['Ls']\n","\n","      ys = model(xs, Ls)\n","\n","      for d in range(3):\n","\n","        preds = ys[d][0]\n","        trues = target[d][0]\n","        m = mask[d]\n","\n","        loss = criterion(preds[m], trues[m])\n","        total_loss += loss.item()\n","\n","        abs_diff = torch.abs(preds[m] - trues[m])\n","        total_abs_error += abs_diff.sum().item()\n","\n","        correct = (abs_diff <= 0.1 * torch.abs(trues[m])).sum().item()\n","        total_correct += correct\n","        total_count += len(m)\n","\n","  accuracy = total_correct / total_count if total_count else 0\n","  mean_abs_error = total_abs_error / total_count if total_count else 0\n","\n","  return total_loss, accuracy, mean_abs_error"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"SsG14ej0WO3B","outputId":"5ee21df6-9cf7-4563-b4da-8f84b96ec1f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Final Evaluation on Validation Set:\n","Validation Loss: 259534.72\n","Validation Accuracy: 82.47%\n","Mean Absolute Error: 1.1433\n"]}],"source":["val_loss, val_acc, val_mae = evaluate(trained_model, val_data, nn.L1Loss(reduction='sum'))\n","\n","print(f\"\\nFinal Evaluation on Validation Set:\")\n","print(f\"Validation Loss: {val_loss:.2f}\")\n","print(f\"Validation Accuracy: {val_acc*100:.2f}%\")\n","print(f\"Mean Absolute Error: {val_mae:.4f}\")"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYdGaUQZ9CcGFRLA768AtC"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}